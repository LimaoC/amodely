{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70d11c28",
   "metadata": {},
   "source": [
    "## All the models and techniques I’ve researched/looked into/developed:\n",
    "- **Simple Linear Regression**\n",
    "- **Clustering algorithms**\n",
    "    - Algorithms like isolation forest work off having lots of dimensions in the data, but this won't work for all features, e.g. conversion rate. **Not possible to retain multiple dimensions and calculate conversion rate since it must be collapsed down to daily, weekly, monthly, etc. along one dimension only**\n",
    "    - Some algorithms, like k-means, don't detect outliers; they just cluster everything into groups\n",
    "    - DBScan looks promising & I might explore it next - but it is very sensitive to initial parameters and clustering techniques in general may not work as effectively on time series data.\n",
    "- **IQR Model**\n",
    "    - Process:\n",
    "        - Fit a high order polynomial to the time series and take the difference to get a stationary time series\n",
    "        - Classify data point as outlier if it is outside of $(Q_1 - 1.5\\text{IQR}, Q_3 + 1.5\\text{IQR})$\n",
    "    - Strengths:\n",
    "        - Easy to implement, quick runtime\n",
    "        - Intuitive\n",
    "    - Limitations:\n",
    "        - No forecasting abilities\n",
    "        - Not a reliable way of detrending data\n",
    "        - Model needs to be recalculated when new data is added\n",
    "        - Anomaly threshold is fixed and doesn't account for finer details in the data\n",
    "- **ARIMA Model**\n",
    "    - Process:\n",
    "        - Split dataset into historical vs recent (last 7 days) data\n",
    "        - Fit ARIMA parameters p, d, q to historical data\n",
    "        - Determine best parameters using grid search of combinations for p, d, q\n",
    "            - \"Best\" as determined by AICc score\n",
    "        - ARIMA forecast a week forward and compare the forecast to the recent data\n",
    "        - Classify data point as outlier if it is outside of the forecast's confidence interval\n",
    "    - Strengths:\n",
    "        - Forecasting\n",
    "        - Seems to work well and detects anomalies appropriately\n",
    "        - Completely automatable with grid search algorithm\n",
    "    - Limitations:\n",
    "        - Fails when data appears just like white noise (i.e. best ARIMA parameters are 0, 0, 0). This may not be that much of a problem given that the anomaly detection is based on the forecast's confidence intervals instead of the actual forecast\n",
    "        - Somewhat slow - takes a few minutes to run. This also may not be a problem if it were set up to run automatically in the background e.g. weekly\n",
    "- **STL Decomposition**\n",
    "    - Process:\n",
    "        - Extract trend and seasonality from time series, leaving behind a (stationary) residual plot\n",
    "        - Perform anomaly detection on the residual plot: could use the IQR method, normality, etc.\n",
    "    - Strengths:\n",
    "        - Extracts both trend & seasonality from plot - definitely stationary\n",
    "    - Limitations:\n",
    "        - No forecasting\n",
    "        - Seems to be prone to overfitting? Trend graph is always very complex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02f51d6",
   "metadata": {},
   "source": [
    "## Proof of concept: anomaly detection algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb63700c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load modules\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from src.amodely import *\n",
    "\n",
    "load_dotenv()\n",
    "DATASET_PATH = os.environ.get(\"DATASET_PATH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadd2ec0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = Amodely(pd.read_excel(DATASET_PATH + \"Conversion Data Extended Period.xlsx\"), measure=\"conversion_rate\")\n",
    "model.detect_anomalies(method=\"arima\", dimension=\"STATE_CODE\", steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2220600",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.download_anomalies()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca86096",
   "metadata": {},
   "source": [
    "## Proof of concept: appending new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93071565",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Original data:\")\n",
    "model2 = Amodely(pd.read_excel(DATASET_PATH + \"Conversion by Day (multi dimension).xlsx\"), measure=\"conversion_rate\")\n",
    "display(model2.df)\n",
    "\n",
    "print(\"New data to be added:\")\n",
    "new_data = pd.read_excel(DATASET_PATH + \"Conversion by Day (small addition).xlsx\")\n",
    "display(new_data)\n",
    "\n",
    "print(\"End result:\")\n",
    "model2.append(new_data, sort_after=False, reset_working=True)\n",
    "display(model2.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb752ab",
   "metadata": {},
   "source": [
    "## Project structure\n",
    "```\n",
    "amodely\n",
    "│   main.py\n",
    "│   requirements.txt\n",
    "│   ...\n",
    "│\n",
    "└───amodely\n",
    "│   │   amodely.py\n",
    "│   │   arimatools.py\n",
    "│   │   lib.py\n",
    "│   │   pipelines.py\n",
    "│   │   ...\n",
    "│   │\n",
    "│   └───tests\n",
    "│       │   ...\n",
    "│   \n",
    "└───docs\n",
    "│   │   ...\n",
    "│\n",
    "└───notebooks\n",
    "│   ...\n",
    "```\n",
    "\n",
    "`admodel.py`\n",
    "- Main file, contains `ADModel` class\n",
    "\n",
    "`arimatools.py`\n",
    "- Provides some helper functions for calculating ARIMA models\n",
    "\n",
    "`lib.py`\n",
    "- Constants\n",
    "\n",
    "`pipelines.py`\n",
    "- Contains all of the data preparation/transformations needed for the model;\n",
    "    - `FillNA` to replace NaNs with zeroes\n",
    "    - `Collapse` to collapse multi-dimensional data down to single-dimensional data\n",
    "    - `FilterCategory` to filter the data using categories\n",
    "    - `AddResponse` to add the response variable (e.g. conversion rate) after data preparation\n",
    "- Integrated with scikit-learn's ecosystem; easy to add new transformations, pipelines, etc. in future if needed, easy to change/modify code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0636ef",
   "metadata": {},
   "source": [
    "## Ideas/moving forward:\n",
    "\n",
    "- Continuous anomaly detection\n",
    "    - Recalculate anomaly detection model and output anomalies as soon as data is loaded in (daily or weekly)\n",
    "    - Model would take a few minutes to run at max, so it wouldn't be too expensive to run daily if needed\n",
    "- Anomaly scores\n",
    "    - Modify models to output \"anomaly scores\" for each anomaly to indicate how likely they are to be an anomaly/how extreme of an anomaly they are\n",
    "        - Logistic regression\n",
    "        - Similar to how an activation function works in neural networks\n",
    "    - This would make it possible to run multiple anomaly detection models. You could then take the average anomaly score for all the data points and flag the data points which have the highest anomaly score (i.e. they keep popping up in all the models)\n",
    "- Presentation\n",
    "    - Save anomalies to a spreadsheet which can then be loaded in PowerBI, or\n",
    "    - Upload anomaly data to a Plotly Dash application on GCP\n",
    "- Scaling to all features\n",
    "    - The model can already handle any dimension for the conversion rate feature\n",
    "    \n",
    "    \n",
    "    \n",
    "Next steps:\n",
    "1. Refine code\n",
    "    - Explore weekly\n",
    "    - Go with STL\n",
    "    - Add new column to store number of standard deviations away from mean (classify how extreme of an anomaly each data point is)\n",
    "2. Anomaly report\n",
    "    - Plotly summary\n",
    "    - Export only points of interest\n",
    "3. Scaling to all measures\n",
    "    - May need to consider what to do with dimensions with a lot of categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10972f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
